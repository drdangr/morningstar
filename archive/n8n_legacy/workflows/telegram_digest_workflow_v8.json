{
  "name": "My workflow 2",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "telegram-posts",
        "options": {}
      },
      "name": "Webhook - Receive Posts",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [
        0,
        -40
      ],
      "id": "7f9bbf95-a6f7-409a-932d-d1b6cb3bb814",
      "webhookId": "telegram-posts-webhook"
    },
    {
      "parameters": {
        "functionCode": "console.log('üì• Processing webhook data...');\nconst data = $json.body || $json;\nconst posts = data.posts || [];\nconsole.log(`üìà Received: ${posts.length} posts`);\nreturn {\n  posts: posts,\n  channels_metadata: data.channels_metadata || {},\n  collection_stats: data.collection_stats || {},\n  timestamp: new Date().toISOString(),\n  has_posts: posts.length > 0\n};"
      },
      "name": "Process & Log Data",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        220,
        -40
      ],
      "id": "61cd4795-9bb4-46fd-ad38-6669c8eedf46"
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.has_posts }}",
              "value2": true
            }
          ]
        }
      },
      "name": "Has Posts?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        440,
        -40
      ],
      "id": "5e59d018-70b1-4562-be3e-a7b740f486a3"
    },
    {
      "parameters": {
        "functionCode": "console.log('ü§ñ Preparing all posts for AI batching...');\nconst data = $json;\nconst posts = data.posts || [];\nconst channelsMetadata = data.channels_metadata || {};\n\n// –°–æ–±–∏—Ä–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤\nconst allActiveCategories = new Set();\nconst channelCategoryMap = {};\nconst categoryDescriptions = {};\n\nObject.keys(channelsMetadata).forEach(channelUsername => {\n    const channelData = channelsMetadata[channelUsername];\n    if (channelData.categories && Array.isArray(channelData.categories)) {\n        const activeCategories = channelData.categories.filter(cat => cat.is_active);\n        channelCategoryMap[channelUsername] = activeCategories;\n        \n        activeCategories.forEach(category => {\n            allActiveCategories.add(category.name);\n            if (category.description) {\n                categoryDescriptions[category.name] = category.description;\n            }\n        });\n    }\n});\n\nconst categories = Array.from(allActiveCategories);\n\n// –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç\nconst topicsDescription = categories.map(cat => {\n    const description = categoryDescriptions[cat] || cat;\n    return `${cat} (${description})`;\n}).join(', ');\n\nconst dynamicPrompt = `–û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π –ø–æ—Å—Ç—ã –ø–æ —Ç–µ–º–∞–º: ${topicsDescription}.\n\n–ü—Ä–∞–≤–∏–ª–æ: summary = \"NULL\" –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –ù–ï –∏–º–µ–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –ù–ò –ö –û–î–ù–û–ô –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Ç–µ–º.\n\n–í–æ–∑–≤—Ä–∞—â–∞–π JSON: {\"results\": [{\"id\": \"post_id\", \"summary\": \"—Ä–µ–∑—é–º–µ –∏–ª–∏ NULL\", \"importance\": 8, \"urgency\": 6, \"significance\": 7, \"category\": \"–¢–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã\"}]}\n\n–í–ê–ñ–ù–û: category –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑: ${categories.join(', ')} –∏–ª–∏ \"NULL\"\n\n–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–æ –°–ú–´–°–õ–£.`;\n\nconsole.log(`üìù Prepared ${posts.length} posts for batching`);\n\nreturn {\n    timestamp: data.timestamp,\n    collection_stats: data.collection_stats,\n    channels_metadata: channelsMetadata,\n    posts: posts,\n    total_posts: posts.length,\n    dynamic_prompt: dynamicPrompt,\n    categories: categories,\n    channel_category_map: channelCategoryMap,\n    batch_processing: true,\n    version: 'v8.1_fixed_categories_endpoint'\n};"
      },
      "name": "Prepare Posts for Batching",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        660,
        -40
      ],
      "id": "35e57c8d-fbe3-4f18-8efb-5ca1dafed0ea"
    },
    {
      "parameters": {
        "batchSize": 30,
        "options": {}
      },
      "name": "Split Posts Into Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        880,
        -60
      ],
      "id": "7621a8d4-6703-4f39-bc5c-598e5dd8656a"
    },
    {
      "parameters": {
        "functionCode": "const currentBatch = $json;\nconst originalData = $('Prepare Posts for Batching').all()[0]?.json;\nconst batchIndex = $runIndex + 1;\nconst totalBatches = Math.ceil(originalData.total_posts / 30);\n\nconsole.log(`üîÑ Processing batch ${batchIndex}/${totalBatches}`);\n\n// –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á –¥–ª—è AI\nconst postsForAI = (Array.isArray(currentBatch) ? currentBatch : [currentBatch]).map(post => ({\n    id: post.id,\n    text: post.text?.substring(0, 1000) || '',\n    channel: post.channel_title,\n    channel_username: post.channel_username,\n    views: post.views || 0,\n    date: post.date,\n    url: post.url\n})).filter(post => post.text.length > 10);\n\nconsole.log(`üìä Batch ${batchIndex}: ${(Array.isArray(currentBatch) ? currentBatch : [currentBatch]).length} input posts ‚Üí ${postsForAI.length} after filter`);\n\nreturn {\n    dynamic_prompt: originalData.dynamic_prompt,\n    posts_for_ai: postsForAI,\n    batch_index: batchIndex,\n    total_batches: totalBatches,\n    total_posts_in_batch: postsForAI.length,\n    channels_metadata: originalData.channels_metadata,\n    channel_category_map: originalData.channel_category_map\n};"
      },
      "name": "Prepare Batch for AI",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1060,
        -260
      ],
      "id": "979a76fe-bbdb-4764-86df-f6aebf7d332e"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o",
          "mode": "list"
        },
        "messages": {
          "values": [
            {
              "content": "={{ $json.dynamic_prompt }}"
            },
            {
              "content": "=–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ {{ $json.total_posts_in_batch }} –ø–æ—Å—Ç–æ–≤ (–±–∞—Ç—á {{ $json.batch_index }}/{{ $json.total_batches }}):\\n\\n{{ JSON.stringify($json.posts_for_ai) }}"
            }
          ]
        },
        "jsonOutput": true,
        "options": {
          "maxTokens": 6000,
          "temperature": 0.3
        }
      },
      "name": "OpenAI API (Batched)",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1,
      "position": [
        1280,
        -260
      ],
      "id": "595372aa-023b-417b-919c-d4456a73663c",
      "credentials": {
        "openAiApi": {
          "id": "RinkWxXeXs9tiXAB",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "functionCode": "const openAIResponse = $json;\nconst batchData = $('Prepare Batch for AI').all()[$runIndex]?.json;\n\nconsole.log(`üìù Processing OpenAI response for batch ${batchData?.batch_index}`);\n\nlet aiAnalysis = [];\nlet parseSuccess = false;\n\n// –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É [0].message.content\nif (Array.isArray(openAIResponse) && openAIResponse[0]?.message?.content) {\n    const contentString = openAIResponse[0].message.content;\n    console.log(`üîç Found content string, length: ${contentString.length}`);\n    \n    try {\n        const parsed = JSON.parse(contentString);\n        aiAnalysis = parsed.results || [];\n        parseSuccess = true;\n        console.log(`‚úÖ Successfully parsed from [0].message.content: ${aiAnalysis.length} results`);\n    } catch (error) {\n        console.log('‚ö†Ô∏è Failed to parse [0].message.content as JSON:', error.message);\n        \n        // –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π JSON\n        try {\n            let fixedContent = contentString;\n            if (!fixedContent.endsWith('}]}}') && !fixedContent.endsWith('}]}')) {\n                const openBraces = (fixedContent.match(/{/g) || []).length;\n                const closeBraces = (fixedContent.match(/}/g) || []).length;\n                const openBrackets = (fixedContent.match(/\\[/g) || []).length;\n                const closeBrackets = (fixedContent.match(/\\]/g) || []).length;\n                \n                for (let i = 0; i < openBraces - closeBraces; i++) {\n                    fixedContent += '}';\n                }\n                for (let i = 0; i < openBrackets - closeBrackets; i++) {\n                    fixedContent += ']';\n                }\n                \n                console.log(`üîß Attempting to fix truncated JSON...`);\n                const fixedParsed = JSON.parse(fixedContent);\n                aiAnalysis = fixedParsed.results || [];\n                parseSuccess = true;\n                console.log(`‚úÖ Fixed truncated JSON: ${aiAnalysis.length} results`);\n            }\n        } catch (fixError) {\n            console.log('‚ùå Failed to fix truncated JSON:', fixError.message);\n        }\n    }\n}\n\n// Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä\nif (!parseSuccess) {\n    if (openAIResponse.message?.content?.results) {\n        aiAnalysis = openAIResponse.message.content.results;\n        console.log(`‚úÖ Found results in message.content.results: ${aiAnalysis.length}`);\n    } else if (openAIResponse.message?.content && typeof openAIResponse.message.content === 'string') {\n        try {\n            const parsed = JSON.parse(openAIResponse.message.content);\n            aiAnalysis = parsed.results || parsed;\n            console.log(`‚úÖ Parsed from string content: ${aiAnalysis.length}`);\n        } catch (error) {\n            console.log('‚ö†Ô∏è Failed to parse string content:', error.message);\n        }\n    }\n}\n\nconsole.log(`‚úÖ Batch ${batchData?.batch_index} processed: ${aiAnalysis.length} AI results`);\n\n// –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–∏—è–Ω–∏—è\nreturn {\n    batch_index: batchData?.batch_index || $runIndex + 1,\n    batch_results: aiAnalysis,\n    batch_posts_count: batchData?.total_posts_in_batch || 0,\n    processed_at: new Date().toISOString(),\n    parse_success: parseSuccess,\n    channels_metadata: batchData?.channels_metadata,\n    channel_category_map: batchData?.channel_category_map\n};"
      },
      "name": "Process Batch Results",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1560,
        80
      ],
      "id": "d1f34e6e-8a7f-4461-8718-361454e59868"
    },
    {
      "parameters": {
        "functionCode": "console.log('üîó Merging all batch results...');\nconst allBatchResults = $input.all();\nconst originalData = $('Prepare Posts for Batching').all()[0]?.json;\n\n// –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\nlet allAIAnalysis = [];\nlet totalProcessedPosts = 0;\n\nallBatchResults.forEach((batchResult, index) => {\n    const batchData = batchResult?.json || {}; // –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç undefined\n    // –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –°–¢–†–û–ö–ê - –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç undefined:\n    console.log(`üì¶ Batch ${batchData.batch_index || index + 1}: ${(batchData.batch_results || []).length} results (parse_success: ${batchData.parse_success || false})`);\n    \n    // –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—Å–∏–≤–∞\n    const batchResults = batchData.batch_results || [];\n    if (Array.isArray(batchResults)) {\n        allAIAnalysis = allAIAnalysis.concat(batchResults);\n    }\n    \n    totalProcessedPosts += batchData.batch_posts_count || 0; // –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω fallback\n});\n\nconsole.log(`‚úÖ Merged ${allAIAnalysis.length} AI results from ${allBatchResults.length} batches`);\n\n// –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ—Å—Ç—ã –ø–æ –∫–∞–Ω–∞–ª–∞–º\nconst posts = originalData?.posts || [];\nconst channelsMetadata = originalData?.channels_metadata || {};\nconst channelCategoryMap = originalData?.channel_category_map || {};\n\nconst groupedPosts = {};\nposts.forEach(post => {\n    const key = post.channel_username || `channel_${post.channel_id}`;\n    if (!groupedPosts[key]) {\n        groupedPosts[key] = {\n            channel_id: post.channel_id,\n            channel_username: post.channel_username,\n            channel_title: post.channel_title,\n            posts: [],\n            categories: channelsMetadata[post.channel_username]?.categories || []\n        };\n    }\n    groupedPosts[key].posts.push(post);\n});\n\n// –ü—Ä–∏–º–µ–Ω—è–µ–º AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Å—Ç–∞–º\nconst processedChannels = {};\n\nObject.keys(groupedPosts).forEach(channelKey => {\n    const channelData = groupedPosts[channelKey];\n    const channelCategories = channelCategoryMap[channelData.channel_username] || [];\n    \n    const processedPosts = channelData.posts.map(post => {\n        const analysis = allAIAnalysis.find(item => item.id == post.id);\n        const isRelevant = analysis && analysis.summary && analysis.summary !== 'NULL';\n        \n        return {\n            ...post,\n            ai_summary: analysis?.summary || post.text?.substring(0, 150) + '...',\n            ai_importance: analysis?.importance || 0,\n            ai_urgency: analysis?.urgency || 0,\n            ai_significance: analysis?.significance || 0,\n            post_category: analysis?.category || 'Unknown',\n            is_relevant: isRelevant,\n            parsing_method: 'correct_batch_processing_v7.3_fixed'\n        };\n    });\n    \n    const relevantPosts = processedPosts.filter(post => post.is_relevant);\n    \n    relevantPosts.sort((a, b) => {\n        const scoreA = a.ai_importance * 3 + a.ai_urgency * 2 + a.ai_significance * 2 + Math.log(a.views || 1);\n        const scoreB = b.ai_importance * 3 + b.ai_urgency * 2 + b.ai_significance * 2 + Math.log(b.views || 1);\n        return scoreB - scoreA;\n    });\n    \n    if (relevantPosts.length > 0) {\n        processedChannels[channelKey] = {\n            ...channelData,\n            posts: relevantPosts.slice(0, 8),\n            all_processed_posts: processedPosts.length,\n            relevant_posts: relevantPosts.length,\n            ai_processed: true,\n            channel_categories: channelCategories.map(c => c.name)\n        };\n    }\n    \n    console.log(`üìä ${channelData.channel_title}: ${processedPosts.length} ‚Üí ${relevantPosts.length} relevant`);\n});\n\nreturn {\n    timestamp: originalData?.timestamp || new Date().toISOString(),\n    processed_at: new Date().toISOString(),\n    stats: originalData?.collection_stats || {},\n    processed_channels: processedChannels,\n    total_channels: Object.keys(processedChannels).length,\n    ai_analysis_stats: {\n        total_analyzed: allAIAnalysis.length,\n        relevant_posts: Object.values(processedChannels).reduce((sum, ch) => sum + ch.relevant_posts, 0),\n        batches_processed: allBatchResults.length,\n        total_posts_processed: totalProcessedPosts,\n        avg_importance: allAIAnalysis.reduce((sum, item) => sum + (item.importance || 0), 0) / Math.max(allAIAnalysis.length, 1),\n        avg_urgency: allAIAnalysis.reduce((sum, item) => sum + (item.urgency || 0), 0) / Math.max(allAIAnalysis.length, 1),\n        avg_significance: allAIAnalysis.reduce((sum, item) => sum + (item.significance || 0), 0) / Math.max(allAIAnalysis.length, 1)\n    },\n    batch_processing_applied: true,\n    relevance_parsing_version: 'v7.3_correct_batch_processing_fixed'\n};"
      },
      "name": "Merge All Batch Results",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1760,
        -40
      ],
      "id": "b7d78037-dac9-4c9d-a8c3-67f141d6954d"
    },
    {
      "parameters": {
        "functionCode": "const processedData = $json;\n\nconst digest = {\n  id: `digest_${Date.now()}`,\n  created_at: processedData.timestamp,\n  processed_at: processedData.processed_at,\n  channels: [],\n  total_posts: 0,\n  batch_processing_applied: true,\n  summary: {\n    channels_processed: Object.keys(processedData.processed_channels).length,\n    original_posts: processedData.stats?.total_posts || 0,\n    relevant_posts: processedData.ai_analysis_stats?.relevant_posts || 0,\n    avg_importance: processedData.ai_analysis_stats?.avg_importance || 0,\n    avg_urgency: processedData.ai_analysis_stats?.avg_urgency || 0,\n    avg_significance: processedData.ai_analysis_stats?.avg_significance || 0,\n    batches_processed: processedData.ai_analysis_stats?.batches_processed || 0\n  }\n};\n\nObject.keys(processedData.processed_channels).forEach(channelKey => {\n  const channelData = processedData.processed_channels[channelKey];\n  \n  digest.channels.push({\n    title: channelData.channel_title,\n    username: channelData.channel_username,\n    categories: channelData.channel_categories || [],\n    posts_count: channelData.posts.length,\n    posts: channelData.posts.map(post => ({\n      title: (post.text || '').substring(0, 100) + '...',\n      url: post.url,\n      views: post.views,\n      date: post.date,\n      ai_importance: post.ai_importance,\n      ai_urgency: post.ai_urgency,\n      ai_significance: post.ai_significance,\n      summary: post.ai_summary,\n      post_category: post.post_category\n    }))\n  });\n  \n  digest.total_posts += channelData.posts.length;\n});\n\nconsole.log(`‚úÖ Correct batch digest ready: ${digest.total_posts} posts, ${digest.summary.batches_processed} batches`);\nreturn digest;"
      },
      "name": "Prepare Digest",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1980,
        -40
      ],
      "id": "41fe4710-644e-4430-96fc-f1f1aa1605b8"
    },
    {
      "parameters": {
        "functionCode": "const digest = $json;\n\nconst backendPayload = {\n  digest_id: digest.id,\n  total_posts: digest.total_posts,\n  channels_processed: digest.summary.channels_processed,\n  original_posts: digest.summary.original_posts,\n  relevant_posts: digest.summary.relevant_posts,\n  avg_importance: digest.summary.avg_importance,\n  avg_urgency: digest.summary.avg_urgency,\n  avg_significance: digest.summary.avg_significance,\n  batch_processing_applied: digest.batch_processing_applied,\n  digest_data: JSON.stringify(digest),\n  processed_at: digest.processed_at\n};\n\nreturn {\n  success: true,\n  digest_id: digest.id,\n  backend_payload: backendPayload,\n  message: `Correct batch digest: ${digest.total_posts} posts, ${digest.summary.batches_processed} batches`,\n  timestamp: new Date().toISOString()\n};"
      },
      "name": "Prepare Backend Payload",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        2200,
        -40
      ],
      "id": "674c81df-4e84-48ad-91ec-afa4c6a5d75e"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://127.0.0.1:8000/api/digests",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json.backend_payload) }}",
        "options": {}
      },
      "name": "Save to Backend API",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        2420,
        -40
      ],
      "id": "1d356fd8-4631-4903-be8c-cbde3be4f77e"
    },
    {
      "parameters": {
        "functionCode": "const apiResponse = $json;\n\nif (apiResponse.digest_id) {\n  console.log('üéâ Correct batch digest saved successfully!');\n  return {\n    success: true,\n    digest_id: apiResponse.digest_id,\n    message: 'Correct batch digest saved!',\n    timestamp: new Date().toISOString()\n  };\n} else {\n  return {\n    success: false,\n    error: 'Failed to save correct batch digest',\n    timestamp: new Date().toISOString()\n  };\n}"
      },
      "name": "Process API Response",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        2640,
        -40
      ],
      "id": "a32b08c2-03b5-43c0-84d4-c6c09cf81e53"
    },
    {
      "parameters": {
        "functionCode": "console.log('‚ö†Ô∏è No posts to process');\nreturn {\n  success: false,\n  message: 'No posts to process',\n  timestamp: new Date().toISOString()\n};"
      },
      "name": "No Posts Handler",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        440,
        260
      ],
      "id": "5b207b5e-d93a-4c71-90df-39d79aced015"
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook - Receive Posts": {
      "main": [
        [
          {
            "node": "Process & Log Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process & Log Data": {
      "main": [
        [
          {
            "node": "Has Posts?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Posts?": {
      "main": [
        [
          {
            "node": "Prepare Posts for Batching",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "No Posts Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Posts for Batching": {
      "main": [
        [
          {
            "node": "Split Posts Into Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Posts Into Batches": {
      "main": [
        [
          {
            "node": "Merge All Batch Results",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Prepare Batch for AI",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Batch for AI": {
      "main": [
        [
          {
            "node": "OpenAI API (Batched)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI API (Batched)": {
      "main": [
        [
          {
            "node": "Process Batch Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Batch Results": {
      "main": [
        [
          {
            "node": "Split Posts Into Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge All Batch Results": {
      "main": [
        [
          {
            "node": "Prepare Digest",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Digest": {
      "main": [
        [
          {
            "node": "Prepare Backend Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Backend Payload": {
      "main": [
        [
          {
            "node": "Save to Backend API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save to Backend API": {
      "main": [
        [
          {
            "node": "Process API Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "42a814e9-838e-4d03-a2e5-fe9b52f161d3",
  "meta": {
    "instanceId": "883bf09e5b1ec169d367de4fbfec1bb4c59c9cdd2c1af7d7b7c29577e28e6ee0"
  },
  "id": "keE93Mjg1MJXmxwf",
  "tags": []
}