#!/usr/bin/env python3
"""
SummarizationServiceCelery - Celery-–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è AI —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ services/summarization.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ Celery
–£—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –±–∞—Ç—á–µ–≤–æ–π —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –∏ –±–∞—Ç—á–µ–≤—ã–π —Ä–µ–∂–∏–º—ã
"""

import json
import re
import time
import logging
import os
import asyncio
from typing import Dict, List, Optional, Any
from openai import OpenAI
from .base_celery import BaseAIServiceCelery

logger = logging.getLogger(__name__)

# üîß –ö–û–ù–¢–†–û–õ–¨ CONCURRENCY: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ OpenAI  
OPENAI_SEMAPHORE = asyncio.Semaphore(2)  # –ú–∞–∫—Å–∏–º—É–º 2 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞


class SummarizationServiceCelery(BaseAIServiceCelery):
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è AI-—Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤ –≤ Celery
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) –∏ –±–∞—Ç—á–µ–≤—ã–π —Ä–µ–∂–∏–º
    """
    
    def __init__(self, settings_manager=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            settings_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö LLM
        """
        super().__init__(settings_manager)
        
        # OpenAI –∫–ª–∏–µ–Ω—Ç –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        self.async_client = None
        
        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –∞—Ç—Ä–∏–±—É—Ç max_summary_length
        self.max_summary_length = 150  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏
        
        logger.info(f"üìù SummarizationServiceCelery –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        if settings_manager:
            logger.info(f"   –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±—É–¥—É—Ç –ø–æ–ª—É—á–µ–Ω—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ SettingsManager")
        else:
            logger.warning(f"   ‚ö†Ô∏è SettingsManager –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω. –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")
    
    async def process_async(self, text: str, language: str = "ru", custom_prompt: Optional[str] = None, 
                max_summary_length: Optional[int] = None, **kwargs) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        try:
            if not text or not text.strip():
                return { "summary": "", "status": "skipped", "reason": "empty_text" }
            
            model, max_tokens, temperature, top_p, settings_max_length = await self._get_model_settings_async()
            
            summary_length = max_summary_length or settings_max_length or self.max_summary_length
            prompt = self._build_single_prompt(custom_prompt, language, summary_length)
            
            # üîß –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú CONCURRENCY –¥–ª—è OpenAI –∑–∞–ø—Ä–æ—Å–æ–≤
            async with OPENAI_SEMAPHORE:
                logger.info(f"üîí –ü–æ–ª—É—á–∏–ª–∏ —Å–ª–æ—Ç –¥–ª—è OpenAI –∑–∞–ø—Ä–æ—Å–∞ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–∞–∫—Ç–∏–≤–Ω—ã—Ö: {2 - OPENAI_SEMAPHORE._value})")
                
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –∏ —è–≤–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º –µ–≥–æ
                from openai import AsyncOpenAI
                async_client = AsyncOpenAI(api_key=self._get_openai_key())
                
                try:
                    response = await async_client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": prompt},
                            {"role": "user", "content": text}
                        ],
                        max_tokens=max_tokens,
                        temperature=temperature,
                        top_p=top_p
                    )
                finally:
                    # –Ø–≤–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å RuntimeError
                    await async_client.close()
                
                logger.info(f"‚úÖ OpenAI –∑–∞–ø—Ä–æ—Å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω, –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å–ª–æ—Ç")
            
            summary = response.choices[0].message.content.strip()
            
            return {
                "summary": summary,
                "language": language,
                "tokens_used": response.usage.total_tokens,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ process_async: {e}")
            return { "summary": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}", "status": "error", "error": str(e) }

    async def process_posts_individually_async(self, posts: List[Dict], bot_id: int, language: str = "ru", 
                                  custom_prompt: Optional[str] = None, **kwargs) -> List[Dict[str, Any]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º)
        """
        logger.info(f"üìù –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è {len(posts)} –ø–æ—Å—Ç–æ–≤")
        
        results = []
        for i, post in enumerate(posts, 1):
            try:
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Backend API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'content', –∞ –Ω–µ 'text'  
                text = post.get('content', '') or post.get('text', '')  # Fallback –Ω–∞ 'text' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                if not text or not text.strip():
                    results.append({
                        "post_id": post.get('id'),
                        "public_bot_id": bot_id,
                        "service_name": "summarization",
                        "status": "skipped",
                        "payload": {"summary": "", "reason": "empty_text"},
                        "metrics": {}
                    })
                    continue
                
                result = await self.process_async(text, language, custom_prompt, **kwargs)
                
                result['post_id'] = post.get('id')
                result['public_bot_id'] = bot_id
                result['service_name'] = 'summarization'

                final_result = {
                    'post_id': result.get('post_id'),
                    'public_bot_id': result.get('public_bot_id'),
                    'service_name': 'summarization',
                    'status': result.get('status', 'success'),
                    'payload': { 'summary': result.get('summary'), 'language': result.get('language') },
                    'metrics': { 'tokens_used': result.get('tokens_used', 0) }
                }
                if result.get('status') == 'error':
                    final_result['payload']['error'] = result.get('error')

                results.append(final_result)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞ {i}: {e}")
                results.append({
                    "post_id": post.get('id'),
                    "public_bot_id": bot_id,
                    "service_name": "summarization",
                    'status': 'success',
                    'payload': {'error': str(e)},
                    'metrics': {}
                })
        
        logger.info(f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results
    
    async def _call_openai_api_async(self, system_prompt: str, user_message: str) -> Optional[str]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç OpenAI API –¥–ª—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        try:
            model, max_tokens, temperature, top_p, settings_max_length = await self._get_model_settings_async()

            # üêû –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –∏ —è–≤–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º –µ–≥–æ
            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=self.openai_api_key)
            
            try:
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    timeout=30
                )
            finally:
                # –Ø–≤–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å RuntimeError
                await client.close()
            
            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Async OpenAI API: {str(e)}")
            return None
        
    async def _get_model_settings_async(self) -> tuple:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ SettingsManager"""
        # üêû FIX: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä—è–º–æ –∑–¥–µ—Å—å
        defaults = {
            'model': 'gpt-4o-mini',
            'max_tokens': 2000,
            'temperature': 0.7,
            'top_p': 1.0,
            'max_summary_length': 150
        }

        if self.settings_manager:
            try:
                config = await self.settings_manager.get_ai_service_config('summarization')
                
                # üîç –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ù–ê–°–¢–†–û–ï–ö
                model = config.get('model', defaults['model'])
                max_tokens = int(config.get('max_tokens', defaults['max_tokens']))
                temperature = float(config.get('temperature', defaults['temperature']))
                top_p = float(config.get('top_p', defaults['top_p']))
                max_summary_length = int(config.get('max_summary_length', defaults['max_summary_length']))
                
                logger.info(f"üìã –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑ SettingsManager:")
                logger.info(f"   ü§ñ –ú–æ–¥–µ–ª—å: {model} (–æ–∂–∏–¥–∞–ª–æ—Å—å: gpt-4o)")
                logger.info(f"   üéØ Max tokens: {max_tokens} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 2000)")
                logger.info(f"   üå°Ô∏è Temperature: {temperature} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 0.7)")
                logger.info(f"   üé≤ Top_p: {top_p} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 1.0)")
                logger.info(f"   üìè Max summary length: {max_summary_length} (fallback: 150)")
                
                return (model, max_tokens, temperature, top_p, max_summary_length)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
                logger.info(f"üìã –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:")
                logger.info(f"   ü§ñ –ú–æ–¥–µ–ª—å: {defaults['model']}")
                logger.info(f"   üéØ Max tokens: {defaults['max_tokens']}")
                logger.info(f"   üå°Ô∏è Temperature: {defaults['temperature']}")
                logger.info(f"   üé≤ Top_p: {defaults['top_p']}")
                logger.info(f"   üìè Max summary length: {defaults['max_summary_length']}")
        else:
            logger.info(f"üìã SettingsManager –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
            logger.info(f"   ü§ñ –ú–æ–¥–µ–ª—å: {defaults['model']}")
            logger.info(f"   üéØ Max tokens: {defaults['max_tokens']}")
            logger.info(f"   üå°Ô∏è Temperature: {defaults['temperature']}")
            logger.info(f"   üé≤ Top_p: {defaults['top_p']}")
            logger.info(f"   üìè Max summary length: {defaults['max_summary_length']}")
        
        return (defaults['model'], defaults['max_tokens'], defaults['temperature'], defaults['top_p'], defaults['max_summary_length'])
    
    def _build_single_prompt(self, custom_prompt: Optional[str], language: str, max_length: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–π —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π
        base_prompt = custom_prompt or self._get_default_prompt(language)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ
        if max_length:
            length_instruction = f"\n\n–õ–∏–º–∏—Ç –¥–ª–∏–Ω—ã: –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è —É–ª–æ–∂–∏—Ç—å—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ {max_length} —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ –Ω–µ –∂–µ—Ä—Ç–≤—É–π –≤–∞–∂–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ —Ä–∞–¥–∏ —ç—Ç–æ–≥–æ. –õ—É—á—à–µ —Å–ª–µ–≥–∫–∞ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç, —á–µ–º –ø–æ—Ç–µ—Ä—è—Ç—å —Å—É—Ç—å –∏–ª–∏ –≤–∫—É—Å —Ç–µ–∫—Å—Ç–∞."
            return f"{base_prompt}{length_instruction}"
        
        return base_prompt
    
    def _build_batch_prompt(self, texts: List[str], custom_prompt: Optional[str], language: str, max_length: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ —Ñ–æ—Ä–º–∞—Ç–µ
        prompt = f"""–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–µ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –°–¢–†–û–ì–û –ü–†–ò–î–ï–†–ñ–ò–í–ê–Ø–°–¨ –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ò–ó –¢–ï–ö–°–¢–û–í –ò–ù–°–¢–†–£–ö–¶–ò–ò –ù–ò–ñ–ï –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞.
–ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–µ "summary".

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
[
  {{"summary": "—Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ 1"}},
  {{"summary": "—Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ 2"}},
  ...
]

–ò–ù–°–¢–†–£–ö–¶–ò–Ø –î–õ–Ø –ö–ê–ñ–î–û–ì–û –¢–ï–ö–°–¢–ê:
{custom_prompt or self._get_default_prompt(language)}

–õ–∏–º–∏—Ç –¥–ª–∏–Ω—ã summary: –ø—Ä–∏–º–µ—Ä–Ω–æ {max_length} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ.

–¢–ï–ö–°–¢–´ –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò:
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏
        for i, text in enumerate(texts, 1):
            prompt += f"\n\n=== –¢–ï–ö–°–¢ {i} ===\n{text}"
        
        return prompt
    
    def _parse_batch_response(self, response_text: str, expected_count: int) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç –±–∞—Ç—á–µ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI"""
        summaries = []
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_text = self._extract_json(response_text)
            if json_text:
                # –ü–∞—Ä—Å–∏–º JSON
                parsed = json.loads(json_text)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º summary –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if isinstance(parsed, list):
                    for item in parsed:
                        if isinstance(item, dict) and 'summary' in item:
                            summaries.append(item['summary'])
                        elif isinstance(item, str):
                            summaries.append(item)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–∞—Ç—á–µ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {e}")
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ summary
        while len(summaries) < expected_count:
            summaries.append(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç {len(summaries) + 1}")
        
        # –û–±—Ä–µ–∑–∞–µ–º –ª–∏—à–Ω–∏–µ
        return summaries[:expected_count]
    
    def _extract_json(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò—â–µ–º –º–∞—Å—Å–∏–≤ JSON
            start = text.find('[')
            if start == -1:
                return ""
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
            bracket_count = 0
            for i in range(start, len(text)):
                if text[i] == '[':
                    bracket_count += 1
                elif text[i] == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        return text[start:i+1]
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON: {e}")
            return ""
    
    def _get_default_prompt(self, language: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        if language == "ru":
            return """–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –æ—Å–Ω–æ–≤–Ω—É—é –∏–¥–µ—é –∏ –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏.
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω—è–π –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏ –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —è–∑—ã–∫
- –ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏
- –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã –∏–ª–∏ –∏–º–µ–Ω–∞ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∞–π –∏—Ö"""
        else:
            return """Create a concise summary of the text, preserving the main idea and key details.
Requirements:
- Write concisely but informatively
- Preserve the main idea and important facts
- Use simple and clear language
- Don't add your own comments or evaluations
- If the text contains specific numbers, dates, or names - be sure to include them"""
    
    def _get_openai_key(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç OpenAI API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required") 