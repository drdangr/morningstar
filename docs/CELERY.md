# üìã –ü–õ–ê–ù –ú–ò–ì–†–ê–¶–ò–ò AI –°–ï–†–í–ò–°–û–í –ù–ê CELERY (Local-First Approach)

## 1. üìù –û–ü–ò–°–ê–ù–ò–ï

–ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º—ã AI –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–æ–≤ —Å custom AI Orchestrator v5 –Ω–∞ Celery - –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á –≤ Python —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ.

**–í–∞–∂–Ω–æ:** –ü–ª–∞–Ω –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ local-first —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –±–µ–∑ Docker. –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏ –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏.

**–¢–µ–∫—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

- `ai_services/orchestrator_v5_parallel.py` - custom event-driven orchestrator —Å AsyncIO
- `CategorizationService` –∏ `SummarizationService` - AI —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- Backend API –∑–∞–ø—É—Å–∫–∞–µ—Ç orchestrator —á–µ—Ä–µ–∑ `subprocess.Popen()`
- Boolean flags (`is_categorized`, `is_summarized`) –¥–ª—è tracking —Å–æ—Å—Ç–æ—è–Ω–∏—è

**–¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

- Celery workers –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á
- Redis –∫–∞–∫ message broker (–ª–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞)
- Flower –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- Backend API –ø—É–±–ª–∏–∫—É–µ—Ç –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å –±–µ–∑ subprocess

## 2. üéØ –û–ë–û–°–ù–û–í–ê–ù–ò–ï

### –†–µ—à–∞–µ–º—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:

**1. –ü—Ä–æ–±–ª–µ–º–∞ subprocess.PIPE** ‚úÖ

- **–ë—ã–ª–æ:** –ó–∞–≤–∏—Å–∞–Ω–∏–µ –ø–æ—Å–ª–µ ~20 –∑–∞–¥–∞—á –∏–∑-–∑–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
- **–°—Ç–∞–Ω–µ—Ç:** Celery workers –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞

**2. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–º–µ—Å—Ç–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π** ‚úÖ

- **–ë—ã–ª–æ:** –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è ‚Üí –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
- **–°—Ç–∞–Ω–µ—Ç:** –ù–∞—Å—Ç–æ—è—â–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ worker pools

**3. –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç** ‚úÖ

- **–ë—ã–ª–æ:** LLM –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –≤ batch —Ä–µ–∂–∏–º–µ
- **–°—Ç–∞–Ω–µ—Ç:** –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –±–∞—Ç—á–∏

**4. –°–ª–æ–∂–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è** ‚úÖ

- **–ë—ã–ª–æ:** –û–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å orchestrator –Ω–∞ –≤—Å–µ –±–æ—Ç—ã
- **–°—Ç–∞–Ω–µ—Ç:** –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ workers

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Celery:

- **Battle-tested** - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ production —Ç—ã—Å—è—á–∞–º–∏ –∫–æ–º–ø–∞–Ω–∏–π
- **Retry –º–µ—Ö–∞–Ω–∏–∑–º—ã** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–±–æ–µ–≤ OpenAI API
- **Rate limiting** - –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞–º–∏ API
- **Monitoring** - Flower dashboard –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- **Persistence** - –∑–∞–¥–∞—á–∏ –Ω–µ —Ç–µ—Ä—è—é—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ

## 3. üìä –ü–û–®–ê–ì–û–í–´–ô –ü–õ–ê–ù –ú–ò–ì–†–ê–¶–ò–ò (LOCAL DEVELOPMENT)

### üîß PHASE 1: Local Infrastructure Setup (1-2 –¥–Ω—è)

#### Task 1.1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Redis –ª–æ–∫–∞–ª—å–Ω–æ

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Redis –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–∞—à–∏–Ω—É
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Redis –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
- [ ] –°–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞/–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Redis
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ Python

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Redis (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –û–°):**

```bash
# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt update
sudo apt install redis-server
sudo systemctl start redis

# Windows (—á–µ—Ä–µ–∑ WSL2 –∏–ª–∏ Redis for Windows)
# –†–µ–∫–æ–º–µ–Ω–¥—É—é WSL2 –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å production
```

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Redis –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:**

```bash
# redis.conf –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
port 6379
bind 127.0.0.1
protected-mode yes
save 900 1
save 300 10
save 60 10000
dbfilename dump.rdb
dir ./redis-data/
```

**–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è:**

```python
# test_redis_connection.py
import redis

r = redis.Redis(host='localhost', port=6379, db=0)
r.set('test_key', 'Hello Celery!')
value = r.get('test_key')
print(f"Redis working: {value}")
```

**Definition of Done:**

- ‚úÖ Redis –∑–∞–ø—É—â–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –ø–æ—Ä—Ç—É 6379
- ‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç —É—Å–ø–µ—à–Ω–æ —á–∏—Ç–∞–µ—Ç/–ø–∏—à–µ—Ç –¥–∞–Ω–Ω—ã–µ
- ‚úÖ Redis —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏
- ‚úÖ –ï—Å—Ç—å —Å–∫—Ä–∏–ø—Ç—ã start_redis.sh / stop_redis.sh

#### Task 1.2: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Celery –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –°–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π virtual environment –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- [ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Celery, Redis, Flower
- [ ] –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Celery
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π worker

**–°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞:**

```bash
# –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
python -m venv venv_celery
source venv_celery/bin/activate  # Windows: venv_celery\Scripts\activate

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install celery[redis]==5.3.4
pip install flower==2.0.1
pip install redis==5.0.1

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip freeze > requirements-celery.txt
```

**–ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Celery:**

```python
# ai_services/celery_app.py
from celery import Celery
import os

# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
app = Celery('digest_bot')

app.conf.update(
    broker_url='redis://localhost:6379/0',
    result_backend='redis://localhost:6379/0',
    
    # –í–∞–∂–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –≤–∏–¥–∏–º –≤—Å–µ –æ—à–∏–±–∫–∏ —Å—Ä–∞–∑—É
    task_eager_propagates=True,
    task_always_eager=False,  # –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –≤ True –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏
    
    # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–∞—á–∞–ª–∞
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    worker_prefetch_multiplier=1,
    task_acks_late=False,
)
```

**–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞:**

```python
# ai_services/test_tasks.py
from .celery_app import app

@app.task
def test_task(x, y):
    """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã Celery"""
    return x + y

@app.task
def test_long_task(duration):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    import time
    time.sleep(duration)
    return f"Slept for {duration} seconds"
```

**–ó–∞–ø—É—Å–∫ worker –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**

```bash
# Terminal 1: –ó–∞–ø—É—Å–∫ worker
cd /path/to/project
celery -A ai_services.celery_app worker --loglevel=debug

# Terminal 2: –ó–∞–ø—É—Å–∫ Flower (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
celery -A ai_services.celery_app flower

# Terminal 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python -c "from ai_services.test_tasks import test_task; result = test_task.delay(2, 3); print(result.get())"
```

**Definition of Done:**

- ‚úÖ Celery worker –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- ‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —É—Å–ø–µ—à–Ω–æ
- ‚úÖ Flower –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:5555
- ‚úÖ –õ–æ–≥–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

### üîÑ PHASE 2: Task Implementation (3-4 –¥–Ω—è)

#### Task 2.1: –°–æ–∑–¥–∞–Ω–∏–µ Celery-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∑–∞–¥–∞—á

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –°–æ–∑–¥–∞—Ç—å `ai_services/tasks.py` —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏
- [ ] –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Celery
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –±–∞—Ç—á–µ–π
- [ ] –î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ retry –ª–æ–≥–∏–∫—É
- [ ] –°–æ–∑–¥–∞—Ç—å –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API

**–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å —É—á–µ—Ç–æ–º –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:**

```python
# ai_services/tasks.py
from celery import Task, group, chain
from celery.utils.log import get_task_logger
from .celery_app import app
from .services.categorization import CategorizationService
from .services.summarization import SummarizationService
from .utils.settings_manager import SettingsManager
import requests
import os
from typing import List, Dict, Any

logger = get_task_logger(__name__)

# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ - –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º URL backend
BACKEND_URL = os.getenv('BACKEND_URL', 'http://localhost:8000')

class BaseAITask(Task):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è AI –∑–∞–¥–∞—á
    –í–∞–∂–Ω–æ: –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º connection pooling,
    —á—Ç–æ–±—ã –ª–µ–≥—á–µ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å
    """
    def __init__(self):
        self.settings_manager = None
        self.service = None
    
    def __call__(self, *args, **kwargs):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        if self.settings_manager is None:
            self.settings_manager = SettingsManager()
        return self.run(*args, **kwargs)

@app.task(base=BaseAITask, bind=True, name='categorize_posts')
def categorize_posts(self, post_ids: List[int], bot_id: int) -> Dict[str, Any]:
    """
    –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±–æ—Ç–∞
    
    Args:
        post_ids: –°–ø–∏—Å–æ–∫ ID –ø–æ—Å—Ç–æ–≤ –∏–∑ posts_cache
        bot_id: ID –ø—É–±–ª–∏—á–Ω–æ–≥–æ –±–æ—Ç–∞
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger.info(f"Starting categorization for {len(post_ids)} posts, bot {bot_id}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π endpoint
        response = requests.get(
            f"{BACKEND_URL}/api/posts/cache",
            params={"limit": 100}  # –í—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ–∫–∞ –Ω–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ IDs
        )
        response.raise_for_status()
        
        all_posts = response.json()["items"]
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –ø–æ—Å—Ç—ã
        posts = [p for p in all_posts if p["id"] in post_ids]
        
        if not posts:
            logger.warning(f"No posts found for IDs: {post_ids}")
            return {"processed": 0, "bot_id": bot_id, "status": "no_posts"}
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –±–æ—Ç–∞
        bot_response = requests.get(f"{BACKEND_URL}/api/public-bots/{bot_id}")
        bot_response.raise_for_status()
        bot_config = bot_response.json()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å
        if self.service is None:
            self.service = CategorizationService(settings_manager=self.settings_manager)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
        # –í–ê–ñ–ù–û: –ó–¥–µ—Å—å –º—ã –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º batch –º–µ—Ç–æ–¥, –∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É
        # –≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        results = []
        for post in posts:
            try:
                result = self.service.process(
                    post, 
                    bot_id,
                    categorization_prompt=bot_config.get("categorization_prompt", "")
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to categorize post {post['id']}: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ API
        if results:
            save_response = requests.put(
                f"{BACKEND_URL}/api/ai/results/batch-status",
                json={
                    "results": results,
                    "bot_id": bot_id,
                    "service": "categorization"
                }
            )
            save_response.raise_for_status()
        
        logger.info(f"Categorized {len(results)} posts for bot {bot_id}")
        return {
            "processed": len(results), 
            "bot_id": bot_id, 
            "status": "success",
            "failed": len(posts) - len(results)
        }
        
    except requests.RequestException as e:
        logger.error(f"API request failed: {e}")
        # Retry —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API
        raise self.retry(exc=e, countdown=60, max_retries=3)
    except Exception as e:
        logger.error(f"Unexpected error in categorization: {e}", exc_info=True)
        # –ù–µ –¥–µ–ª–∞–µ–º retry –¥–ª—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ - –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –∫–æ–¥
        return {"processed": 0, "bot_id": bot_id, "status": "error", "error": str(e)}

@app.task(bind=True, name='summarize_posts')
def summarize_posts(self, post_ids: List[int], bot_id: int, 
                   categorization_result: Dict = None) -> Dict[str, Any]:
    """
    –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤
    
    –í–∞–∂–Ω–æ: —ç—Ç–∞ –∑–∞–¥–∞—á–∞ –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ —á–∞—Å—Ç—å chain –ø–æ—Å–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏,
    –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
    """
    logger.info(f"Starting summarization for posts, bot {bot_id}")
    
    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –ª–æ–≥–∏–∫–∞, –Ω–æ —Å —É—á–µ—Ç–æ–º is_categorized —Ñ–ª–∞–≥–∞
    # ...
    
    return {"processed": len(post_ids), "bot_id": bot_id, "status": "success"}

@app.task(name='process_bot_digest')
def process_bot_digest(bot_id: int, limit: int = None) -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è orchestration –∑–∞–¥–∞—á–∞
    
    –≠—Ç–∞ –∑–∞–¥–∞—á–∞ –∑–∞–º–µ–Ω—è–µ—Ç —Ç–≤–æ–π AI Orchestrator v5
    """
    logger.info(f"Processing digest for bot {bot_id}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ—Ç–∞
        bot_response = requests.get(f"{BACKEND_URL}/api/public-bots/{bot_id}")
        bot_response.raise_for_status()
        bot_config = bot_response.json()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –±–æ—Ç–∞ –∏–ª–∏ default
        posts_limit = limit or bot_config.get('max_posts_per_digest', 15)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã
        # TODO: –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å endpoint –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ bot_id
        response = requests.get(
            f"{BACKEND_URL}/api/posts/unprocessed",
            params={"limit": posts_limit}
        )
        response.raise_for_status()
        posts = response.json()["items"]
        
        if not posts:
            logger.info(f"No unprocessed posts for bot {bot_id}")
            return {"bot_id": bot_id, "status": "no_posts"}
        
        # –ö–õ–Æ–ß–ï–í–û–ï –†–ï–®–ï–ù–ò–ï: —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –±–∞—Ç—á–∏
        # –≠—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–∞—Ö
        OPTIMAL_BATCH_SIZE = 3  # –ù–∞—á–Ω–µ–º —Å 3, –ø–æ—Ç–æ–º –ø–æ–¥–±–µ—Ä–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫–∏ –∑–∞–¥–∞—á
        workflows = []
        for i in range(0, len(posts), OPTIMAL_BATCH_SIZE):
            batch = posts[i:i+OPTIMAL_BATCH_SIZE]
            batch_ids = [p["id"] for p in batch]
            
            # –ö–∞–∂–¥—ã–π –±–∞—Ç—á –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ:
            # –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è -> —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            workflow = chain(
                categorize_posts.s(batch_ids, bot_id),
                summarize_posts.s(batch_ids, bot_id)
            )
            workflows.append(workflow)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ü–µ–ø–æ—á–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        job = group(workflows).apply_async()
        
        return {
            "bot_id": bot_id,
            "status": "processing",
            "total_posts": len(posts),
            "batches": len(workflows),
            "batch_size": OPTIMAL_BATCH_SIZE,
            "job_id": str(job.id) if job.id else None
        }
        
    except Exception as e:
        logger.error(f"Failed to process bot digest: {e}", exc_info=True)
        return {"bot_id": bot_id, "status": "error", "error": str(e)}

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
@app.task(name='test_ai_service')
def test_ai_service(service_type: str = 'categorization') -> Dict[str, Any]:
    """–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
    try:
        if service_type == 'categorization':
            service = CategorizationService()
        else:
            service = SummarizationService()
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ—Å—Ç
        test_post = {
            "id": 999,
            "content": "Test content for AI service validation",
            "channel_id": 1
        }
        
        result = service.process(test_post, bot_id=1)
        return {"status": "success", "result": result}
    except Exception as e:
        return {"status": "error", "error": str(e)}
```

**Definition of Done:**

- ‚úÖ –ó–∞–¥–∞—á–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- ‚úÖ –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ –ª–æ–≥–∏—Ä—É–µ—Ç —Å–≤–æ—é —Ä–∞–±–æ—Ç—É –ø–æ–¥—Ä–æ–±–Ω–æ
- ‚úÖ Retry –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
- ‚úÖ –ë–∞—Ç—á–∏ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä

**–ü–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**

```python
# test_local_tasks.py
import pytest
from ai_services.tasks import categorize_posts, process_bot_digest

def test_categorize_single_batch():
    """–¢–µ—Å—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –±–∞—Ç—á–∞"""
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    result = categorize_posts.apply(args=([1, 2, 3], 4)).get()
    
    assert result["status"] == "success"
    assert result["processed"] > 0
    print(f"Processed: {result['processed']} posts")

def test_optimal_batch_size():
    """–¢–µ—Å—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞"""
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
    for batch_size in [1, 3, 5, 10]:
        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞–º–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è –∏ –∫–∞—á–µ—Å—Ç–≤–æ
        pass

def test_error_handling():
    """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫"""
    # –¢–µ—Å—Ç —Å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ ID
    result = categorize_posts.apply(args=([99999], 1)).get()
    assert result["status"] in ["no_posts", "error"]
```

#### Task 2.2: Backend API –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –°–æ–∑–¥–∞—Ç—å feature flag –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è Celery
- [ ] –î–æ–±–∞–≤–∏—Ç—å endpoints –¥–ª—è Celery –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π orchestrator
- [ ] –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π
- [ ] –°–æ–∑–¥–∞—Ç—å debug endpoints –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

**Backend –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å feature flag:**

```python
# backend/main.py
import os
from typing import Optional

# Feature flag –¥–ª—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏
USE_CELERY = os.getenv("USE_CELERY", "false").lower() == "true"
CELERY_DEBUG = os.getenv("CELERY_DEBUG", "true").lower() == "true"

# –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ Celery –≤–∫–ª—é—á–µ–Ω
if USE_CELERY:
    try:
        from celery import Celery
        celery_app = Celery('backend', broker='redis://localhost:6379/0')
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ —á—Ç–æ–±—ã Celery –∏—Ö –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª
        from ai_services.tasks import process_bot_digest, categorize_posts
        
        CELERY_AVAILABLE = True
    except ImportError:
        logger.warning("Celery not installed, falling back to legacy orchestrator")
        CELERY_AVAILABLE = False
else:
    CELERY_AVAILABLE = False

@app.post("/api/ai/orchestrator-commands")
async def handle_orchestrator_command(command: dict):
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π endpoint —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º
    """
    logger.info(f"Orchestrator command: {command}, USE_CELERY={USE_CELERY}")
    
    if command["action"] == "start_background":
        if USE_CELERY and CELERY_AVAILABLE:
            return await _handle_celery_start(command)
        else:
            return await _handle_legacy_start(command)
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ

async def _handle_celery_start(command: dict):
    """–ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Celery"""
    try:
        active_bots = await get_active_bots()
        tasks_created = []
        
        for bot in active_bots:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –∏–º–ø–æ—Ä—Ç—ã –µ—Å–ª–∏ Celery –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            from ai_services.tasks import process_bot_digest
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            task = process_bot_digest.delay(bot.id)
            tasks_created.append({
                "bot_id": bot.id,
                "task_id": str(task.id),
                "status": "queued"
            })
            
            logger.info(f"Created Celery task {task.id} for bot {bot.id}")
        
        return {
            "status": "started",
            "mode": "celery",
            "tasks": tasks_created,
            "debug": CELERY_DEBUG
        }
    except Exception as e:
        logger.error(f"Celery start failed: {e}")
        if CELERY_DEBUG:
            # –í debug —Ä–µ–∂–∏–º–µ –ø–∞–¥–∞–µ–º —Å –æ—à–∏–±–∫–æ–π
            raise
        else:
            # –í production –¥–µ–ª–∞–µ–º fallback
            logger.warning("Falling back to legacy orchestrator")
            return await _handle_legacy_start(command)

async def _handle_legacy_start(command: dict):
    """–°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ subprocess"""
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ —Å subprocess.Popen
    # ...
    return {"status": "started", "mode": "legacy"}

# Debug endpoints –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
if CELERY_DEBUG:
    @app.get("/api/debug/celery-status")
    async def debug_celery_status():
        """–ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ç—É—Å Celery –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        if not CELERY_AVAILABLE:
            return {"error": "Celery not available"}
        
        from celery.task.control import inspect
        i = inspect()
        
        return {
            "active": i.active(),
            "scheduled": i.scheduled(),
            "reserved": i.reserved(),
            "stats": i.stats(),
            "registered": i.registered_tasks(),
            "conf": {
                "broker": celery_app.conf.broker_url,
                "backend": celery_app.conf.result_backend
            }
        }
    
    @app.post("/api/debug/test-celery-task")
    async def test_celery_task(bot_id: int = 1):
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        from ai_services.tasks import test_ai_service
        
        task = test_ai_service.delay()
        return {
            "task_id": str(task.id),
            "status": "queued",
            "get_result_command": f"celery -A ai_services.celery_app result {task.id}"
        }
```

**Definition of Done:**

- ‚úÖ Backend —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å Celery, —Ç–∞–∫ –∏ –±–µ–∑ –Ω–µ–≥–æ
- ‚úÖ Feature flag –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏
- ‚úÖ Debug endpoints –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- ‚úÖ –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

### üìä PHASE 3: –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ª–∞–¥–∫–∞ (3-4 –¥–Ω—è)

#### Task 3.1: End-to-End —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –°–æ–∑–¥–∞—Ç—å –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É 1, 5, 10, 50 –ø–æ—Å—Ç–æ–≤
- [ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å legacy orchestrator
- [ ] –ò–∑–º–µ—Ä–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –Ω–∞–π—Ç–∏ bottlenecks
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–µ–π

**–°–∫—Ä–∏–ø—Ç –¥–ª—è E2E —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**

```python
# test_e2e_celery.py
import time
import requests
from datetime import datetime

class E2ETest:
    def __init__(self):
        self.backend_url = "http://localhost:8000"
        self.results = []
    
    def test_batch_sizes(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π"""
        for batch_size in [1, 3, 5, 10]:
            print(f"\n--- Testing batch size: {batch_size} ---")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            os.environ["OPTIMAL_BATCH_SIZE"] = str(batch_size)
            
            # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º Celery worker
            self._restart_celery_worker()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            start_time = time.time()
            response = requests.post(
                f"{self.backend_url}/api/ai/orchestrator-commands",
                json={"action": "start_background"}
            )
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            self._wait_for_completion()
            
            duration = time.time() - start_time
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = self._collect_metrics()
            
            self.results.append({
                "batch_size": batch_size,
                "duration": duration,
                "success_rate": metrics["success_rate"],
                "avg_time_per_post": metrics["avg_time_per_post"]
            })
    
    def compare_with_legacy(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å legacy orchestrator"""
        # –¢–µ—Å—Ç —Å legacy
        os.environ["USE_CELERY"] = "false"
        legacy_result = self._run_test("legacy")
        
        # –¢–µ—Å—Ç —Å Celery
        os.environ["USE_CELERY"] = "true"
        celery_result = self._run_test("celery")
        
        print("\n=== COMPARISON ===")
        print(f"Legacy: {legacy_result['duration']:.2f}s")
        print(f"Celery: {celery_result['duration']:.2f}s")
        print(f"Speedup: {legacy_result['duration'] / celery_result['duration']:.2f}x")
```

#### Task 3.2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Flower –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- [ ] –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus —Ñ–æ—Ä–º–∞—Ç–µ
- [ ] –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞—Ç—å memory usage
- [ ] –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers
- [ ] –°–æ–∑–¥–∞—Ç—å dashboard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

**–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```bash
#!/bin/bash
# start_monitoring.sh

# –ó–∞–ø—É—Å–∫ Redis —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
redis-cli CONFIG SET latency-monitor-threshold 100
redis-cli --latency

# –ó–∞–ø—É—Å–∫ Flower —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏
celery -A ai_services.celery_app flower \
    --port=5555 \
    --broker_api=redis://localhost:6379/0 \
    --max_tasks=10000 \
    --persistent=True \
    --db=flower.db

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ worker'–æ–≤
watch -n 1 'ps aux | grep celery'
```

### üöÄ PHASE 4: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (2-3 –¥–Ω—è)

#### Task 4.1: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π BATCH_SIZE –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–æ–≤
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å connection pooling –¥–ª—è requests
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
- [ ] –£–º–µ–Ω—å—à–∏—Ç—å overhead –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–∞—Ç—á–∞—Ö

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è production:**

```python
# ai_services/optimizations.py
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Connection pooling –¥–ª—è API calls
session = requests.Session()
retry = Retry(
    total=3,
    read=3,
    connect=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504)
)
adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
session.mount('http://', adapter)
session.mount('https://', adapter)

# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
OPTIMAL_SETTINGS = {
    "batch_size": 5,  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ —Ç–µ—Å—Ç–∞–º–∏
    "worker_concurrency": 4,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö workers
    "prefetch_multiplier": 1,  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
    "task_time_limit": 300,  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º –Ω–∞ –∑–∞–¥–∞—á—É
    "task_soft_time_limit": 240,  # Warning –ø–æ—Å–ª–µ 4 –º–∏–Ω—É—Ç
}
```

#### Task 4.2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ production

**–ü–æ–¥–∑–∞–¥–∞—á–∏:**

- [ ] –°–æ–∑–¥–∞—Ç—å production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ deployment
- [ ] –°–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- [ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å rollback –ø—Ä–æ—Ü–µ–¥—É—Ä—É
- [ ] –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ production –¥–∞–Ω–Ω—ã—Ö

### üê≥ PHASE 5: Docker Integration (–ø–æ—Å–ª–µ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏)

**–≠—Ç–æ –±—É–¥–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç—Ç–∞–ø –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏:**

- –°–æ–∑–¥–∞–Ω–∏–µ Dockerfile –¥–ª—è Celery workers
- Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ç–µ–π –∏ volumes
- Integration —Ç–µ—Å—Ç—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö

## 4. üìà –ú–ï–¢–†–ò–ö–ò –£–°–ü–ï–•–ê

**–ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ (—Ü–µ–ª–µ–≤—ã–µ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):**

- ‚¨áÔ∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ 100 –ø–æ—Å—Ç–æ–≤: —Å ~160 —Å–µ–∫ –¥–æ ~40 —Å–µ–∫ (4x —É–ª—É—á—à–µ–Ω–∏–µ)
- ‚¨ÜÔ∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: —Å 1 –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ 4-8 workers
- ‚¨áÔ∏è Failure rate: —Å –∑–∞–≤–∏—Å–∞–Ω–∏–π –¥–æ <1% —Å auto-retry
- ‚¨ÜÔ∏è Throughput: 500+ –ø–æ—Å—Ç–æ–≤/—á–∞—Å –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ

**–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ:**

- ‚úÖ –ù–µ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏–π –ø—Ä–∏ –ª—é–±–æ–º –æ–±—ä–µ–º–µ
- ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞ –æ—Ç–ª–∞–¥–∫–∏ —á–µ—Ä–µ–∑ Flower UI
- ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
- ‚úÖ –õ–µ–≥–∫–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á

## 5. üö® –†–ò–°–ö–ò –ò –ú–ò–¢–ò–ì–ê–¶–ò–Ø

**–†–∏—Å–∫ 1: –ü—Ä–æ–±–ª–µ–º—ã —Å Redis –Ω–∞ Windows**

- **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å WSL2 –∏–ª–∏ Docker —Ç–æ–ª—å–∫–æ –¥–ª—è Redis

**–†–∏—Å–∫ 2: –†–∞–∑–ª–∏—á–∏—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ dev/prod**

- **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è, —Ç–µ –∂–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫

**–†–∏—Å–∫ 3: –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–ª–∞–¥–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á**

- **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, Flower UI

## 6. ‚è±Ô∏è TIMELINE

**–û–±—â–µ–µ –≤—Ä–µ–º—è:** 9-12 –¥–Ω–µ–π (–±–µ–∑ Docker)

1. **Phase 1:** Local Infrastructure (1-2 –¥–Ω—è)
2. **Phase 2:** Implementation (3-4 –¥–Ω—è)
3. **Phase 3:** Testing & Debugging (3-4 –¥–Ω—è)
4. **Phase 4:** Optimization (2-3 –¥–Ω—è)
5. **Phase 5:** Docker (–æ—Ç–¥–µ–ª—å–Ω–æ, –ø–æ—Å–ª–µ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏)

**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å:** –ù–µ–ª—å–∑—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å - –∫–∞–∂–¥–∞—è —Ñ–∞–∑–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π