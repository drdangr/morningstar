{
  "name": "Telegram Digest Bot Workflow - Early Batching (Corrected)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "telegram-posts",
        "responseMode": "responseNode",
        "options": {}
      },
      "name": "Webhook - Receive Posts",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [-1280, -200],
      "id": "webhook-node",
      "webhookId": "telegram-posts-webhook"
    },
    {
      "parameters": {
        "functionCode": "const data = $json;\n\nconsole.log('üì® Received data structure:', Object.keys(data));\nconsole.log('üìä Total posts:', data.posts?.length || 0);\n\nif (data.posts && data.posts.length > 0) {\n  console.log('üìà Sample post structure:', Object.keys(data.posts[0]));\n}\n\nreturn data;"
      },
      "name": "Process & Log Data",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [-1060, -200],
      "id": "process-log-node"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.posts && $json.posts.length > 0 }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equal"
              }
            }
          ],
          "combineOperation": "any"
        },
        "looseTypeValidation": true
      },
      "name": "Has Posts?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [-840, -200],
      "id": "has-posts-node"
    },
    {
      "parameters": {
        "batchSize": 30,
        "options": {}
      },
      "name": "Split Posts Into Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [-620, -200],
      "id": "split-batches-node"
    },
    {
      "parameters": {
        "functionCode": "const currentBatch = $json;\nconst originalData = $('Has Posts?').first().json;\n\nconsole.log(`üìã Processing batch ${$item(0).$index + 1} with ${Array.isArray(currentBatch) ? currentBatch.length : 1} posts`);\n\n// –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞\nconst batchPosts = Array.isArray(currentBatch) ? currentBatch : [currentBatch];\n\n// –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –ø–æ –∫–∞–Ω–∞–ª–∞–º –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞\nconst groupedByChannel = {};\nconst channelCategoryMap = {};\n\n// –°–æ–∑–¥–∞–µ–º –º–∞–ø—É –∫–∞–Ω–∞–ª–æ–≤ –∏ –∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\nif (originalData.channels_metadata && Array.isArray(originalData.channels_metadata)) {\n  originalData.channels_metadata.forEach(channel => {\n    if (channel.categories && Array.isArray(channel.categories)) {\n      channelCategoryMap[channel.username] = channel.categories;\n    }\n  });\n}\n\n// –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –ø–æ –∫–∞–Ω–∞–ª–∞–º\nbatchPosts.forEach(post => {\n  const channelKey = post.channel || 'unknown';\n  if (!groupedByChannel[channelKey]) {\n    groupedByChannel[channelKey] = {\n      channel_title: post.channel_title || channelKey,\n      channel_username: channelKey,\n      posts: []\n    };\n  }\n  groupedByChannel[channelKey].posts.push(post);\n});\n\n// –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è AI\nconst allCategories = new Set();\nObject.values(channelCategoryMap).forEach(categories => {\n  categories.forEach(cat => allCategories.add(cat.name));\n});\n\nconst topicsDescription = Array.from(allCategories).join(', ');\n\n// –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å—Ç—ã –¥–ª—è AI\nconst postsForAI = batchPosts.map(post => ({\n  id: post.id,\n  text: post.text,\n  channel: post.channel,\n  views: post.views || 0,\n  date: post.date\n}));\n\nconst dynamicPrompt = `–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –ø–æ—Å—Ç—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Å–ª–µ–¥—É—é—â–∏–º —Ç–µ–º–∞–º: ${topicsDescription}.\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –æ–¥–Ω–æ–π –∏–∑ —Ç–µ–º, —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –æ—Ü–µ–Ω–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º. –ï—Å–ª–∏ –ø–æ—Å—Ç –ù–ï –∏–º–µ–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –ù–ò –ö –û–î–ù–û–ô –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Ç–µ–º, —É–∫–∞–∂–∏ summary –∫–∞–∫ \"NULL\".\n\n–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:\n{\n  \"results\": [\n    {\n      \"id\": \"post_id\",\n      \"summary\": \"–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ NULL\",\n      \"importance\": —á–∏—Å–ª–æ_–æ—Ç_1_–¥–æ_10,\n      \"urgency\": —á–∏—Å–ª–æ_–æ—Ç_1_–¥–æ_10,\n      \"significance\": —á–∏—Å–ª–æ_–æ—Ç_1_–¥–æ_10,\n      \"category\": \"—Ç–æ—á–Ω–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_—Ç–µ–º—ã_–∏–∑_—Å–ø–∏—Å–∫–∞\"\n    }\n  ]\n}`;\n\nreturn {\n  batch_index: $item(0).$index + 1,\n  posts_count: batchPosts.length,\n  grouped_channels: groupedByChannel,\n  channel_category_map: channelCategoryMap,\n  topics_description: topicsDescription,\n  posts_for_ai: postsForAI,\n  dynamic_prompt: dynamicPrompt,\n  timestamp: originalData.timestamp || new Date().toISOString()\n};"
      },
      "name": "Prepare Batch for AI",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [-400, -200],
      "id": "prepare-batch-node"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o",
          "mode": "list"
        },
        "messages": {
          "values": [
            {
              "content": "={{ $json.dynamic_prompt }}"
            },
            {
              "content": "=–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ {{ $json.posts_count }} –ø–æ—Å—Ç–æ–≤ (–±–∞—Ç—á {{ $json.batch_index }}):\\n\\n{{ JSON.stringify($json.posts_for_ai) }}"
            }
          ]
        },
        "jsonOutput": true,
        "options": {
          "maxTokens": 6000,
          "temperature": 0.3
        }
      },
      "name": "OpenAI API (Batched)",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1,
      "position": [-180, -200],
      "id": "openai-batched-node",
      "credentials": {
        "openAiApi": {
          "id": "RinkWxXeXs9tiXAB",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "functionCode": "const openAIResponse = $json;\nconst batchData = $('Prepare Batch for AI').first().json;\n\nconsole.log(`üìù Processing OpenAI response for batch ${batchData.batch_index}`);\n\nlet aiAnalysis = [];\nlet parseSuccess = false;\n\n// –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É [0].message.content\nif (Array.isArray(openAIResponse) && openAIResponse[0]?.message?.content) {\n  const contentString = openAIResponse[0].message.content;\n  console.log(`üîç Found content string, length: ${contentString.length}`);\n  \n  try {\n    const parsed = JSON.parse(contentString);\n    aiAnalysis = parsed.results || [];\n    parseSuccess = true;\n    console.log(`‚úÖ Successfully parsed from [0].message.content: ${aiAnalysis.length} results`);\n  } catch (error) {\n    console.log('‚ö†Ô∏è Failed to parse [0].message.content as JSON:', error.message);\n    \n    // –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π JSON\n    try {\n      let fixedContent = contentString;\n      if (!fixedContent.endsWith('}]}}') && !fixedContent.endsWith('}]}')) {\n        const openBraces = (fixedContent.match(/{/g) || []).length;\n        const closeBraces = (fixedContent.match(/}/g) || []).length;\n        const openBrackets = (fixedContent.match(/\\[/g) || []).length;\n        const closeBrackets = (fixedContent.match(/\\]/g) || []).length;\n        \n        for (let i = 0; i < openBraces - closeBraces; i++) {\n          fixedContent += '}';\n        }\n        for (let i = 0; i < openBrackets - closeBrackets; i++) {\n          fixedContent += ']';\n        }\n        \n        console.log(`üîß Attempting to fix truncated JSON...`);\n        const fixedParsed = JSON.parse(fixedContent);\n        aiAnalysis = fixedParsed.results || [];\n        parseSuccess = true;\n        console.log(`‚úÖ Fixed truncated JSON: ${aiAnalysis.length} results`);\n      }\n    } catch (fixError) {\n      console.log('‚ùå Failed to fix truncated JSON:', fixError.message);\n    }\n  }\n}\n\n// Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä\nif (!parseSuccess) {\n  if (openAIResponse.message?.content?.results) {\n    aiAnalysis = openAIResponse.message.content.results;\n    console.log(`‚úÖ Found results in message.content.results: ${aiAnalysis.length}`);\n  } else if (openAIResponse.message?.content && typeof openAIResponse.message.content === 'string') {\n    try {\n      const parsed = JSON.parse(openAIResponse.message.content);\n      aiAnalysis = parsed.results || parsed;\n      console.log(`‚úÖ Parsed from string content: ${aiAnalysis.length}`);\n    } catch (error) {\n      console.log('‚ö†Ô∏è Failed to parse string content:', error.message);\n    }\n  }\n}\n\n// –ü—Ä–∏–º–µ–Ω—è–µ–º AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫ –ø–æ—Å—Ç–∞–º –ø–æ –∫–∞–Ω–∞–ª–∞–º\nconst processedChannels = {};\nconst channelCategoryMap = batchData.channel_category_map || {};\n\nObject.keys(batchData.grouped_channels).forEach(channelKey => {\n  const channelData = batchData.grouped_channels[channelKey];\n  \n  const processedPosts = channelData.posts.map(post => {\n    const analysis = aiAnalysis.find(item => item.id == post.id);\n    const isRelevant = analysis && analysis.summary && analysis.summary !== 'NULL';\n    \n    return {\n      ...post,\n      ai_summary: analysis?.summary || post.text?.substring(0, 150) + '...',\n      ai_importance: analysis?.importance || 0,\n      ai_urgency: analysis?.urgency || 0,\n      ai_significance: analysis?.significance || 0,\n      post_category: analysis?.category || 'Unknown',\n      is_relevant: isRelevant,\n      parsing_method: 'early_batching_corrected_v1'\n    };\n  });\n  \n  const relevantPosts = processedPosts.filter(post => post.is_relevant);\n  \n  // –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏\n  relevantPosts.sort((a, b) => {\n    const scoreA = a.ai_importance * 3 + a.ai_urgency * 2 + a.ai_significance * 2 + Math.log(a.views || 1);\n    const scoreB = b.ai_importance * 3 + b.ai_urgency * 2 + b.ai_significance * 2 + Math.log(b.views || 1);\n    return scoreB - scoreA;\n  });\n  \n  if (relevantPosts.length > 0) {\n    processedChannels[channelKey] = {\n      ...channelData,\n      posts: relevantPosts.slice(0, 8),\n      all_processed_posts: processedPosts.length,\n      relevant_posts: relevantPosts.length,\n      ai_processed: true,\n      channel_categories: (channelCategoryMap[channelData.channel_username] || []).map(c => c.name)\n    };\n  }\n  \n  console.log(`üìä ${channelData.channel_title}: ${processedPosts.length} ‚Üí ${relevantPosts.length} relevant`);\n});\n\nconsole.log(`‚úÖ Batch ${batchData.batch_index} processed: ${Object.keys(processedChannels).length} channels with content`);\n\nreturn {\n  batch_index: batchData.batch_index,\n  processed_channels: processedChannels,\n  ai_analysis_stats: {\n    total_analyzed: aiAnalysis.length,\n    relevant_posts: Object.values(processedChannels).reduce((sum, ch) => sum + ch.relevant_posts, 0),\n    avg_importance: aiAnalysis.reduce((sum, item) => sum + (item.importance || 0), 0) / Math.max(aiAnalysis.length, 1),\n    avg_urgency: aiAnalysis.reduce((sum, item) => sum + (item.urgency || 0), 0) / Math.max(aiAnalysis.length, 1),\n    avg_significance: aiAnalysis.reduce((sum, item) => sum + (item.significance || 0), 0) / Math.max(aiAnalysis.length, 1)\n  },\n  timestamp: batchData.timestamp,\n  parse_success: parseSuccess\n};"
      },
      "name": "Process Batch Results",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [40, -200],
      "id": "process-batch-results-node"
    },
    {
      "parameters": {
        "functionCode": "console.log('üîó Merging all batch results...');\nconst allBatchResults = $input.all();\n\nlet allProcessedChannels = {};\nlet totalStats = {\n  total_analyzed: 0,\n  relevant_posts: 0,\n  avg_importance: 0,\n  avg_urgency: 0,\n  avg_significance: 0,\n  batches_processed: allBatchResults.length\n};\n\nconst firstBatch = allBatchResults[0]?.json;\nconst timestamp = firstBatch?.timestamp || new Date().toISOString();\n\n// –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –±–∞—Ç—á–µ–π\nallBatchResults.forEach((batchResult, index) => {\n  const batchData = batchResult.json;\n  console.log(`üì¶ Merging batch ${batchData.batch_index}: ${Object.keys(batchData.processed_channels || {}).length} channels`);\n  \n  // –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–∞–Ω–∞–ª—ã\n  Object.keys(batchData.processed_channels || {}).forEach(channelKey => {\n    const channelData = batchData.processed_channels[channelKey];\n    \n    if (!allProcessedChannels[channelKey]) {\n      allProcessedChannels[channelKey] = {\n        channel_title: channelData.channel_title,\n        channel_username: channelData.channel_username,\n        channel_categories: channelData.channel_categories,\n        posts: [],\n        all_processed_posts: 0,\n        relevant_posts: 0,\n        ai_processed: true\n      };\n    }\n    \n    // –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Å—Ç—ã\n    allProcessedChannels[channelKey].posts = allProcessedChannels[channelKey].posts.concat(channelData.posts || []);\n    allProcessedChannels[channelKey].all_processed_posts += channelData.all_processed_posts || 0;\n    allProcessedChannels[channelKey].relevant_posts += channelData.relevant_posts || 0;\n  });\n  \n  // –°—É–º–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n  if (batchData.ai_analysis_stats) {\n    totalStats.total_analyzed += batchData.ai_analysis_stats.total_analyzed || 0;\n    totalStats.relevant_posts += batchData.ai_analysis_stats.relevant_posts || 0;\n    totalStats.avg_importance += batchData.ai_analysis_stats.avg_importance || 0;\n    totalStats.avg_urgency += batchData.ai_analysis_stats.avg_urgency || 0;\n    totalStats.avg_significance += batchData.ai_analysis_stats.avg_significance || 0;\n  }\n});\n\n// –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\nif (totalStats.batches_processed > 0) {\n  totalStats.avg_importance /= totalStats.batches_processed;\n  totalStats.avg_urgency /= totalStats.batches_processed;\n  totalStats.avg_significance /= totalStats.batches_processed;\n}\n\n// –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ—Å—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–∞–Ω–∞–ª–µ\nObject.keys(allProcessedChannels).forEach(channelKey => {\n  const channel = allProcessedChannels[channelKey];\n  channel.posts.sort((a, b) => {\n    const scoreA = a.ai_importance * 3 + a.ai_urgency * 2 + a.ai_significance * 2 + Math.log(a.views || 1);\n    const scoreB = b.ai_importance * 3 + b.ai_urgency * 2 + b.ai_significance * 2 + Math.log(b.views || 1);\n    return scoreB - scoreA;\n  });\n  \n  // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤\n  channel.posts = channel.posts.slice(0, 8);\n});\n\nconsole.log(`‚úÖ Merged ${totalStats.batches_processed} batches: ${Object.keys(allProcessedChannels).length} channels, ${totalStats.relevant_posts} relevant posts`);\n\nreturn {\n  timestamp: timestamp,\n  processed_at: new Date().toISOString(),\n  processed_channels: allProcessedChannels,\n  total_channels: Object.keys(allProcessedChannels).length,\n  ai_analysis_stats: totalStats,\n  batch_processing_applied: true,\n  relevance_parsing_version: 'early_batching_corrected_v1'\n};"
      },
      "name": "Merge All Batches",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [260, -200],
      "id": "merge-all-node"
    },
    {
      "parameters": {
        "functionCode": "const processedData = $json;\n\nconst digest = {\n  id: `digest_${Date.now()}`,\n  created_at: processedData.timestamp,\n  processed_at: processedData.processed_at,\n  channels: [],\n  total_posts: 0,\n  batch_processing_applied: true,\n  summary: {\n    channels_processed: Object.keys(processedData.processed_channels).length,\n    original_posts: processedData.ai_analysis_stats?.total_analyzed || 0,\n    relevant_posts: processedData.ai_analysis_stats?.relevant_posts || 0,\n    avg_importance: processedData.ai_analysis_stats?.avg_importance || 0,\n    avg_urgency: processedData.ai_analysis_stats?.avg_urgency || 0,\n    avg_significance: processedData.ai_analysis_stats?.avg_significance || 0,\n    batches_processed: processedData.ai_analysis_stats?.batches_processed || 0\n  }\n};\n\nObject.keys(processedData.processed_channels).forEach(channelKey => {\n  const channelData = processedData.processed_channels[channelKey];\n  \n  digest.channels.push({\n    title: channelData.channel_title,\n    username: channelData.channel_username,\n    categories: channelData.channel_categories || [],\n    posts_count: channelData.posts.length,\n    posts: channelData.posts.map(post => ({\n      title: (post.text || '').substring(0, 100) + '...',\n      url: post.url,\n      views: post.views,\n      date: post.date,\n      ai_importance: post.ai_importance,\n      ai_urgency: post.ai_urgency,\n      ai_significance: post.ai_significance,\n      summary: post.ai_summary,\n      post_category: post.post_category\n    }))\n  });\n  \n  digest.total_posts += channelData.posts.length;\n});\n\nconsole.log(`‚úÖ Corrected early batching digest ready: ${digest.total_posts} posts from ${digest.summary.batches_processed} batches`);\nreturn digest;"
      },
      "name": "Prepare Digest",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [480, -200],
      "id": "prepare-digest-node"
    },
    {
      "parameters": {
        "functionCode": "const digest = $json;\n\nconst backendPayload = {\n  digest_id: digest.id,\n  total_posts: digest.total_posts,\n  channels_processed: digest.summary.channels_processed,\n  original_posts: digest.summary.original_posts,\n  relevant_posts: digest.summary.relevant_posts,\n  avg_importance: digest.summary.avg_importance,\n  avg_urgency: digest.summary.avg_urgency,\n  avg_significance: digest.summary.avg_significance,\n  batch_processing_applied: digest.batch_processing_applied,\n  digest_data: JSON.stringify(digest),\n  processed_at: digest.processed_at\n};\n\nreturn {\n  success: true,\n  digest_id: digest.id,\n  backend_payload: backendPayload,\n  message: `Corrected early batching digest: ${digest.total_posts} posts`,\n  timestamp: new Date().toISOString()\n};"
      },
      "name": "Prepare Backend Payload",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [700, -200],
      "id": "prepare-backend-node"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://127.0.0.1:8000/api/digests",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json.backend_payload) }}"
      },
      "name": "Save to Backend API",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [920, -200],
      "id": "save-backend-node"
    },
    {
      "parameters": {
        "functionCode": "const apiResponse = $json;\n\nif (apiResponse.digest_id) {\n  console.log('üéâ Corrected early batching digest saved successfully!');\n  return {\n    success: true,\n    digest_id: apiResponse.digest_id,\n    message: 'Corrected early batching digest saved!',\n    timestamp: new Date().toISOString()\n  };\n} else {\n  return {\n    success: false,\n    error: 'Failed to save corrected early batching digest',\n    timestamp: new Date().toISOString()\n  };\n}"
      },
      "name": "Process API Response",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1140, -200],
      "id": "process-api-response-node"
    },
    {
      "parameters": {
        "functionCode": "console.log('‚ö†Ô∏è No posts to process');\nreturn {\n  success: false,\n  message: 'No posts to process',\n  timestamp: new Date().toISOString()\n};"
      },
      "name": "No Posts Handler",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [-840, 100],
      "id": "no-posts-node"
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook - Receive Posts": {
      "main": [
        [
          {
            "node": "Process & Log Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process & Log Data": {
      "main": [
        [
          {
            "node": "Has Posts?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Posts?": {
      "main": [
        [
          {
            "node": "Split Posts Into Batches",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "No Posts Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Posts Into Batches": {
      "main": [
        [
          {
            "node": "Prepare Batch for AI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Merge All Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Batch for AI": {
      "main": [
        [
          {
            "node": "OpenAI API (Batched)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI API (Batched)": {
      "main": [
        [
          {
            "node": "Process Batch Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Batch Results": {
      "main": [
        [
          {
            "node": "Merge All Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge All Batches": {
      "main": [
        [
          {
            "node": "Prepare Digest",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Digest": {
      "main": [
        [
          {
            "node": "Prepare Backend Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Backend Payload": {
      "main": [
        [
          {
            "node": "Save to Backend API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save to Backend API": {
      "main": [
        [
          {
            "node": "Process API Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "early-batching-corrected-v1",
  "meta": {
    "instanceId": "early-batching-corrected-workflow"
  },
  "id": "EarlyBatchingCorrectedWorkflow",
  "tags": []
} 